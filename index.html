<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriately as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Towards Understanding Why Label Smoothing Degrades Selective Classification and How to Fix It. Accepted to ICLR 2025.">
  <meta property="og:title" content="Towards Understanding Why Label Smoothing Degrades Selective Classification and How to Fix It" />
  <meta property="og:description" content="Don't forget about majority voting when you evaluate your TTA method :)" />
  <meta property="og:url" content="https://farinamatteo.github.io/zero/" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/teaser.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="Towards Understanding Why Label Smoothing Degrades Selective Classification and How to Fix It. Accepted to ICLR 2025.">
  <meta name="twitter:description" content="Towards Understanding Why Label Smoothing Degrades Selective Classification and How to Fix It">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/teaser.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Selective Classification, Label-smoothing, Uncertainty Quantification, ICLR 2025">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Towards Understanding Why Label Smoothing Degrades Selective Classification and How to Fix It</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

  <!-- and it's easy to individually load additional languages -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

  <script>hljs.highlightAll();</script>


  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>

  <style>
    .author-block {
      margin-right: 10px;
      /* Adjust the value as per your preference */
    }
  </style>

  <style>
    /* Custom CSS for tooltip */
    .custom-tooltip .tooltip-inner {
      background-color: #f1f1f1;
      color: #333333;
    }

    .custom-tooltip .tooltip.bs-tooltip-top .arrow::before {
      border-top-color: #f1f1f1;
    }
  </style>

  <style>
    .image-row {
        display: flex;
        justify-content: space-between;
        width: 100%;
        padding-top: 10px;
        padding-bottom: 10px;
        position: relative; /* To allow positioning of superscripts */
    }
    .image-container {
        height: 14vh; /* Set image height relative to viewport */
        width: 30%;
        position: relative; /* To position the superscript relative to this container */
        margin: 10px; /* Add some margin between images */
    }
    .image-container img {
        width: 100%; /* Make sure the image fills the container */
        height: 100%; /* Fill container height */
        /* object-fit: cover; Maintain aspect ratio */
    }
    .superscript {
        position: absolute;
        top: -5%; /* Adjust the position relative to the container */
        left: -5%; /* Place it near the top-right corner */
        font-size: 1.2rem; /* Set font size for the superscript */
        color: black; /* Choose a suitable color */
    }
  </style>




  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- Title here -->
            <h1 class="title is-1 publication-title">Towards Understanding Why Label Smoothing Degrades Selective Classification and How to Fix It</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=wLOCfQgAAAAJ&hl=en" target="_blank">Guoxuan Xia</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="https://github.com/o-laurent" target="_blank">Olivier Laurent</a><sup>2, 3</sup>
              </span>
              <span class="author-block">
                <a href="https://giannifranchi.github.io/" target="_blank">Gianni Franchi</a><sup>2</sup>
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=A-XGmNoAAAAJ&hl=en" target="_blank">Christos-Savvas Bouganis</a><sup>1</sup>
              </span>
              <span class="author-block">
            </div>

            <!-- Institution Logos Here -->
             <div class="image-row">
              <div class="image-container">
                <img src="static/images/imperial.svg" alt="Imperial" />
                <span class="superscript">1</span>
              </div>
              <div class="image-container">
                <img src="static/images/ensta.svg" alt="ENSTA" />
                <span class="superscript">2</span>
              </div>
              <div class="image-container">
                <img src="static/images/ups.svg" alt="UniversitÃ© Paris-Saclay"/>
                <span class="superscript">3</span>
              </div>
             </div>

            <div class="column has-text-centered">
              <div class="publication-links">

            <!-- Github link -->
            <span class="link-block">
              <a href="https://github.com/ENSTA-U2IS-AI/Label-smoothing-Selective-classification" target="_blank"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fab fa-github"></i>
              </span>
              <span>Code</span>
            </a>

            <!-- ArXiv abstract Link -->
            <span class="link-block">
              <a href="https://arxiv.org/abs/2403.14715" target="_blank"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="ai ai-arxiv"></i>
              </span>
              <span>arXiv</span>
            </a>

            <!-- Github link -->
            <span class="link-block">
              <a href="https://huggingface.co/ENSTA-U2IS/Label-smoothing-Selective-classification" target="_blank"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fa-solid fa-square-binary"></i>
              </span>
              <span>Models</span>
            </a>
          </span>
          <p><br>Correspondence to: g[dot]xia21[at]imperial[dot]ac[dot]uk</p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
                Label smoothing (LS) is a popular regularisation method for training neural networks as it is effective in improving test accuracy and is simple to implement. 
                Hard one-hot labels are smoothed by uniformly distributing probability mass to other classes, reducing overfitting. 
                Prior work has suggested that in some cases LS can degrade selective classification (SC) -- where the aim is to reject misclassifications using a model's uncertainty. 
                In this work, we first demonstrate empirically across an extended range of large-scale tasks and architectures that LS consistently degrades SC. 
                We then address a gap in existing knowledge, providing an explanation for this behaviour by analysing logit-level gradients: 
                LS degrades the uncertainty rank ordering of correct vs incorrect predictions by regularising the max logit more when a prediction is likely to be correct, and less when it is likely to be wrong. 
                This elucidates previously reported experimental results where strong classifiers underperform in SC. We then demonstrate the empirical effectiveness of post-hoc logit normalisation for recovering lost SC performance caused by LS. 
                Furthermore, linking back to our gradient analysis, we again provide an explanation for why such normalisation is effective. <br><br>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract  -->

  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body has-text-centered">
        <h2 class="title is-3">Takeaways</h2>
        <div style="display: flex; justify-content: center;">
          <img src="static/images/teaser.png" alt="Banner Image" height="100%" width="75%" style="margin-bottom: 55px;">
        </div> -->
        <!-- <h2 class="subtitle has-text-justified">
          <br><b>Background on Marginal Entropy Minimization.</b> Test-Time Adaptation aims at adapting a model to a single image at inference time.
          Ideally, no form of prior or external knowledge should be employed in doing so.
          An established paradigm for TTA is <b>M</b>arginal <b>E</b>ntropy <b>M</b>inimization, which works by augmenting the image N times, 
          computing the so-called "marginal probability distribution" (i.e., the average probability distribution over the views), and 
          minimizing the entropy of this distribution.<br><br>

          <b>Findings.</b> We find that the argmax of the marginal distribution is invariant to <b>MEM</b> most of the time (and can be guaranteed to be so under certain conditions), 
          and that this marginal distribution itself is reasonably better than standard inference, under the assumption that the model is well-calibrated.
          <br><br>Empirical evidence for these findings is shown below (left: invariance, right: ensemble verification).
        </h2>
        <img src="static/images/I_binned_ent_vs_invariance.png" alt="Banner Image" style="height: 18em; width: auto; padding-inline: 2rem;">
        <img src="static/images/ensemble_verification_over_datasets.png" alt="Banner Image" style="height: 18em; width: auto; padding-inline: 2rem">

        <h2 class="subtitle has-text-justified">
          <b>Problem.</b> Calibration is missing on augmented data, but we largely observe that CLIP models are still pretty accurate in this regime.
          For example, here is what the reliability plots of CLIP-ViT-B-16 look like.
        </h2>
        <img src="static/images/rpI.png" alt="Banner Image" style="height: 20em; width: auto;">

        <h2 class="subtitle has-text-justified">
          <b>TTA with "zero" temperature</b> is a direct consequence of these observations: since confidence information is unreliable, 
          simply compute the marginal distribution <i>after</i> the temperature has been zeroed-out! By only adapting this parameter, we are effectively marginalizing
          across one-hot encoded vectors... does this remind you of something?
        </h2>

      </div>
    </div>
  </section>  -->
  <!-- End paper abstract -->

  <!-- Method overview-->
  <!-- <section class="section hero is-light">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Implementation</h2>
      <h2 class="subtitle has-text-centered" style="padding: 0px; margin: 0px">ZERO is implemented in a few lines of code. You can find a PyTorch-like implementation right here :)</h2>
      <pre class="has-text-justified" style="width: 80rem; overflow-x: auto; padding: 0px; margin: 0px">
      <code class="python" style="padding: 0px; margin: 0px">
        def zero(image, z_txt, N, gamma, temp):
          """
          :param z_txt: pre-computed text embeddings (C,hdim)
          :param temp: modelâs original temperature
          :param augment: takes (C,H,W) and returns (N,C,H,W)
          :param gamma: filtering percentile (e.g., 0.3)
          """
          views = augment(image, num_views=N) # generate augmented views
          l = model.image_encoder(views) @ z_txt.t() # predict (unscaled logits)
          l_filt = confidence_filter(l, temp, top=gamma) # retain most confident preds
          zero_temp = torch.finfo(l_filt.dtype).eps # zero temperature
          p_bar = (l_filt / zero_temp).softmax(dim=1).sum(dim=0) # marginalize
          return p_bar.argmax()
      </code>
      </pre>
    </div>
  </section> -->
  <!-- End method overview -->

    <!-- Results -->
    <!-- <section class="hero">
      <div class="container is-max-desktop">
        <div class="hero-body has-text-centered">
          <h2 class="title is-3">Results</h2>
          <h2 class="subtitle has-text-justified">
            <br>We evaluate ZERO on the standard TTA benchmarks, including robustness to Natural Distribution Shifts and Fine-grained Classification.
            The results below report CLIP-ViT-B-16 from OpenAI, and compare ZERO to TPT, PromptAlign and RLCF.
          </h2>
  
          <p><b>Robustness to Natural Distribution Shifts</b></p>
          <img src="static/images/nds.png" alt="Banner Image" style="height: auto; width: 100em;">
          <br><br>

          <p><b>Fine-grained Classification</b></p>
          <img src="static/images/fg.png" alt="Banner Image" style="height: auto; width: 100em;">

          <h2 class="subtitle has-text-justified"><br>
            We find that ZERO, in all its simplicity, establishes a new <b>state-of-the-art</b> in TTA! 
            Don't forget about majority voting when you evaluate your TTA method!! :)
          </h2>

        </div>
      </div>
    </section> -->
    <!-- Results -->

    <!-- Acknowledgements -->
    <section class="hero is-light">
      <div class="container is-max-desktop">
        <div class="hero-body has-text-centered">
          <h2 class="title is-3">Acknowledgements</h2>
          <h2 class="subtitle has-text-justified">
            <br>Guoxuan Xiaâs PhD is funded jointly by Arm and the EPSRC. This work was performed using HPC resources from GENCI-IDRIS (Grant 2023-
            [AD011011970R3]). We thank the anonymous reviewers for their valuable feedback.
          </h2>
        </div>
      </div>
    </section>
    <!-- Acknowledgements -->

    <!--BibTex citation -->
    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
          <h2 class="title">BibTeX</h2>
          <pre style="padding: 0px; margin: 0px"><code style="overflow-x: auto; padding: 0px; margin: 0px">@inproceedings{xia2024understanding,
    title={Towards Understanding Why Label Smoothing Degrades Selective Classification and How to Fix It},
    author={Xia, Guoxuan and Laurent, Olivier and Franchi, Gianni and Bouganis, Christos-Savvas},
    booktitle={ICLR},
    year={2025}       
}</code></pre>
        </div>
    </section>
  <!--End BibTex citation -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.10.2/umd/popper.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.0/js/bootstrap.min.js"></script>

  <script>
    $(function () {
      $('[data-toggle="tooltip"]').tooltip();
    });
  </script>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a>.
              You are free to borrow the code of this website, we just ask that you link back to this page in the footer.
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>


</body>

</html>